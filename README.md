# Image-Captioning-AI
An AI system that generates natural language descriptions for images using computer vision and NLP techniques.


Project Brief: Image Captioning AI
üéØ Objective

Build an AI system that takes an image as input and automatically generates a descriptive caption, combining computer vision and natural language processing (NLP).

| Component                       | Tools/Models                                                              |
| ------------------------------- | ------------------------------------------------------------------------- |
| üß† **Image Feature Extraction** | Pre-trained CNN (e.g. VGG16, ResNet50 from `torchvision` or `TensorFlow`) |
| üìù **Caption Generation**       | RNN (LSTM) or Transformer (e.g. GPT-2, BERT-style decoder)                |
| üì¶ **Dataset**                  | COCO, Flickr8k/Flickr30k, or custom                                       |
| üñºÔ∏è **Interface**               | Jupyter Notebook or Web UI (optional)                                     |

‚öôÔ∏è Workflow

Input Image

Extract Features using a CNN (like ResNet)

Feed Features to a Decoder (LSTM or Transformer)

Generate Caption

Display Result

üß† Sample Implementation (Minimal Prototype)

Below is a simplified image captioning pipeline using TensorFlow, ResNet50, and a pre-trained model from keras.applications with a dummy decoder (to simulate captioning).

üêç Requirements

Install via pip:

pip install tensorflow numpy pillow

üìÑ Python Example (Prototype)
import numpy as np
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
import tensorflow as tf
import matplotlib.pyplot as plt

# Load ResNet50 model pre-trained on ImageNet
model = ResNet50(weights='imagenet')

def load_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    plt.imshow(img)
    plt.axis('off')
    plt.show()
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    return preprocess_input(img_array)

def predict_caption(img_path):
    processed_img = load_image(img_path)
    preds = model.predict(processed_img)
    decoded = decode_predictions(preds, top=1)[0]
    label = decoded[0][1]  # top-1 predicted label
    caption = f"This image likely contains a {label}."
    return caption

# Example usage
img_path = 'example.jpg'  # replace with your own image
caption = predict_caption(img_path)
print("Caption:", caption)


üìù Note: This is not true captioning, but a classification-to-caption prototype for fast demos. True captioning requires training a CNN + RNN pipeline or using a pre-trained transformer-based model.

üöÄ Advanced Option: Use Pretrained Transformer Captioning Models

Use Hugging Face‚Äôs pre-trained image captioning models:

üî• Example using BLIP from Hugging Face
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests
import torch

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

img = Image.open("your_image.jpg").convert("RGB")
inputs = processor(img, return_tensors="pt")
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)

print("Caption:", caption)


Install requirements:

pip install transformers torch torchvision pillow

Thankyou
